{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47857880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - General\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "\n",
    "# Statistics and Model Evaluation\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import expon, reciprocal\n",
    "\n",
    "# Visualisation for Machine Learning\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Neural Networks and Deep Learning\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "import kerastuner as kt\n",
    "\n",
    "# Model Interpretability\n",
    "import shap\n",
    "\n",
    "# Local application/library specific imports\n",
    "from collections import defaultdict\n",
    "from kedro.pipeline import node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_kedro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = catalog.load(\"X_train_main\")\n",
    "y_train = catalog.load(\"y_train_main\")\n",
    "X_validate = catalog.load(\"X_validate_main\")\n",
    "y_validate = catalog.load(\"y_validate_main\")\n",
    "decision_tree_params = catalog.load(\"params:decision_tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af247ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation ##########################################################\n",
    "\n",
    "def print_model_name(model_name):\n",
    "    print(f\"Evaluation Metrics: {model_name}\")\n",
    "\n",
    "def calculate_accuracy(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "\n",
    "def store_and_print_classification_report(y_test, y_pred):\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(f\"Classification Report:\\n{report}\")\n",
    "    return report\n",
    "\n",
    "def print_and_return_confusion_matrix(y_test, y_pred):\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion Matrix:\\n{matrix}\")\n",
    "    return matrix\n",
    "\n",
    "def print_and_return_f1_score(y_test, y_pred):\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    return f1\n",
    "\n",
    "def print_and_return_precision(y_test, y_pred):\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print(f\"Precision: {precision}\")\n",
    "    return precision\n",
    "\n",
    "def print_and_return_recall(y_test, y_pred):\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print(f\"Recall: {recall}\")\n",
    "    return recall\n",
    "\n",
    "def print_auc(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Prints the AUC for the given model and test data.\n",
    "    \"\"\"\n",
    "    # Probabilities for the positive class\n",
    "    probas = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, probas)\n",
    "\n",
    "    print(f\"AUC: {roc_auc}\")\n",
    "    return roc_auc\n",
    "\n",
    "def print_auc_tf(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Prints the AUC for the given model and test data.\n",
    "    \"\"\"\n",
    "    # Probabilities for the positive class\n",
    "    probas = model.predict(X_test).ravel()\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, probas)\n",
    "\n",
    "    print(f\"AUC: {roc_auc}\")\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def get_best_hyperparameters_decision_tree(grid_search): # Main Support\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(best_params)\n",
    "    best_hyperparameters_df = pd.DataFrame(best_params, index=[0])\n",
    "\n",
    "    return best_hyperparameters_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree_with_grid_search(X_train, y_train, X_validate, y_validate, model_name): # Main Function   \n",
    "\n",
    "    param_grid = {\n",
    "        \"decisiontreeclassifier__max_depth\": [20],\n",
    "        \"decisiontreeclassifier__min_samples_split\": range(10, 38),\n",
    "        \"decisiontreeclassifier__min_samples_leaf\": range(33, 43),\n",
    "        \"decisiontreeclassifier__criterion\": [\"entropy\"]\n",
    "    }\n",
    "\n",
    "    pipeline = make_pipeline_imb(StandardScaler(), SMOTE(random_state=42), \n",
    "                                 DecisionTreeClassifier(random_state=42))\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring='accuracy', verbose=1)\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    predictions = best_model.predict(X_validate)\n",
    "\n",
    "    print_model_name(model_name)\n",
    "    accuracy = calculate_accuracy(y_validate, predictions)\n",
    "    auc = print_auc(best_model, X_validate, y_validate)\n",
    "    f1 = print_and_return_f1_score(y_validate, predictions)\n",
    "    precision = print_and_return_precision(y_validate, predictions)\n",
    "    recall = print_and_return_recall(y_validate, predictions)\n",
    "\n",
    "    \n",
    "\n",
    "    # Get hyperparameter ranges\n",
    "    best_params_df = get_best_hyperparameters_decision_tree(grid_search)\n",
    "    confusion_matrix_values = print_and_return_confusion_matrix(y_validate, predictions)\n",
    "#     f1 = print_and_return_f1_score(y_validate, predictions)\n",
    "#     precision = print_and_return_precision(y_validate, predictions)\n",
    "#     recall = print_and_return_recall(y_validate, predictions)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix_values.ravel()\n",
    "\n",
    "\n",
    "    # Store actual max depth \n",
    "    actual_max_depth = grid_search.best_estimator_.named_steps['decisiontreeclassifier']\n",
    "    actual_max_depth = actual_max_depth.tree_.max_depth\n",
    "\n",
    "    print('Decision Tree depth:', actual_max_depth)\n",
    "    \n",
    "\n",
    "\n",
    "    extracted_model = best_model.named_steps['decisiontreeclassifier']\n",
    "    explainer = shap.Explainer(extracted_model, X_train)\n",
    "    \n",
    "    \n",
    "    shap_values = explainer(X_train)\n",
    "    print(type(shap_values))\n",
    "    print(shap_values.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    return {\n",
    "        'shap_values': shap_values,\n",
    "        'expected_value': explainer.expected_value\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_decision_tree_with_grid_search(X_train, y_train, X_validate, y_validate, \"decision_tree\")\n",
    "shap_values = results['shap_values']\n",
    "expected_value = results['expected_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380dc84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = shap_values.values.shape[-1]\n",
    "for i in range(num_classes):\n",
    "    class_shap_values = shap_values.values[..., i]\n",
    "    print(f\"Class {i} SHAP Values:\")\n",
    "    shap.summary_plot(class_shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb56131",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_shap_values = shap_values\n",
    "saved_expected_value = expected_value\n",
    "\n",
    "instance_index = 0  \n",
    "class_index = 0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that have numpy arrays for the SHAP values\n",
    "if isinstance(saved_shap_values, shap.Explanation):\n",
    "    saved_shap_values_array = saved_shap_values.values # Access values if its an explanation object \n",
    "else:\n",
    "    saved_shap_values_array = saved_shap_values\n",
    "\n",
    "# Pick the expected value for the class \n",
    "expected_value_for_class = saved_expected_value[class_index] if isinstance(saved_expected_value, np.ndarray) else saved_expected_value\n",
    "\n",
    "# Pick the SHAP values for the specific instance and class\n",
    "shap_values_for_instance_and_class = saved_shap_values_array[instance_index, :, class_index]\n",
    "\n",
    "# plot\n",
    "shap.decision_plot(expected_value_for_class, shap_values_for_instance_and_class, X_train.iloc[instance_index, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec80433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (joel_mashana_dissertation_project)",
   "language": "python",
   "name": "kedro_joel_mashana_dissertation_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
